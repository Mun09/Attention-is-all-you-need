{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqqcpwRBitRCLqRrVtfc5z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ebdca4ad712643679bd8a9fb74675d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Sentence:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5135a6ab51a74fc591c99e8402256ca2",
            "placeholder": "Enter a sentence",
            "style": "IPY_MODEL_46fca8cde343483cb89c26a3ba6cd3af",
            "value": "저는 지금 굉장히 배고프고 밥을 먹기를 원하는 상태입니다."
          }
        },
        "5135a6ab51a74fc591c99e8402256ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46fca8cde343483cb89c26a3ba6cd3af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc0fe602d26c4ad19f81c6c4679fb65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "영어로 번역하기",
            "disabled": false,
            "icon": "check",
            "layout": "IPY_MODEL_71ee3478fd014677a43a8703c4ae6c79",
            "style": "IPY_MODEL_d9d23ac237ad4891ba311746d795b11c",
            "tooltip": "Click me"
          }
        },
        "71ee3478fd014677a43a8703c4ae6c79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9d23ac237ad4891ba311746d795b11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mun09/Attention-is-all-you-need/blob/main/Attention_is_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 논문 제목: Attention Is All You Need\n",
        "## 논문 구현 프로젝트\n",
        "### 구현자: JaeYoung Moon\n",
        "\n",
        "이 노트북은 논문 \"Attention Is All You Need\"의 구현을 다루며, 해당 논문의 주요 알고리즘과 KoreantoEnglish 번역실험을 재현합니다.\n"
      ],
      "metadata": {
        "id": "t9FBEBpOQOrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#마운팅"
      ],
      "metadata": {
        "id": "i-sJgP4aYU-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrveKy0yhq6v",
        "outputId": "5160b113-cb1b-4bec-95ef-ca3439a4cd68"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##필수 라이브러리 설치"
      ],
      "metadata": {
        "id": "usIEIyp7QYdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# if device == 'cpu':\n",
        "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
        "# else:\n",
        "#   !pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu117\n",
        "!pip install torch\n",
        "!pip install torchtext==0.13.1\n",
        "!pip install spacy==3.3.0\n",
        "!pip install pandas\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zc4lQ0RBRsge"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ai-hub 한국어문장-영어문장 20만개\n",
        "data_path = '/content/drive/MyDrive/dataset.xlsx'\n",
        "data = pd.read_excel(data_path)\n",
        "\n",
        "print(data.head)\n",
        "print(data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgEPWPJfpQhb",
        "outputId": "c8f5b7c7-f25f-402d-f1f0-e7f3e5d1e7d6"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of            SID                                                 원문  \\\n",
            "0            1  'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 ...   \n",
            "1            2                                       씨티은행에서 일하세요?   \n",
            "2            3              푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.   \n",
            "3            4   11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.   \n",
            "4            5     6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.   \n",
            "...        ...                                                ...   \n",
            "199995  199996                               나는 먼저 청소기로 바닥을 밀었어요.   \n",
            "199996  199997                             나는 먼저 팀 과제를 하고 놀러 갔어요.   \n",
            "199997  199998                              나는 비 같은 멋진 연예인을 좋아해요.   \n",
            "199998  199999                           나는 멋진 자연 경치를 보고 눈물을 흘렸어.   \n",
            "199999  200000                               나는 멋진 중학교 생활을 기대합니다.   \n",
            "\n",
            "                                                      번역문  \n",
            "0       Bible Coloring' is a coloring application that...  \n",
            "1                             Do you work at a City bank?  \n",
            "2       PURITO's bestseller, which recorded 4th rough ...  \n",
            "3       In Chapter 11 Jesus called Lazarus from the to...  \n",
            "4       I would feel grateful to know how many stocks ...  \n",
            "...                                                   ...  \n",
            "199995                First of all, I vacuumed the floor.  \n",
            "199996  I did the team assignment first and went out t...  \n",
            "199997                 I like cool entertainer like Rain.  \n",
            "199998                I cried seeing the amazing scenery.  \n",
            "199999  I look forward to a great middle school experi...  \n",
            "\n",
            "[200000 rows x 3 columns]>\n",
            "(200000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "n = data.shape[0]\n",
        "\n",
        "if not os.path.isfile('/content/drive/MyDrive/dataset_ko.txt'):\n",
        "  f = open('/content/drive/MyDrive/dataset_ko.txt', 'w', encoding='utf-8')\n",
        "  for i in range(n):\n",
        "    f.write(data.iloc[i, 1] + '\\n')\n",
        "  f.close()\n",
        "\n",
        "if not os.path.isfile('/content/drive/MyDrive/dataset_en.txt'):\n",
        "  f = open('/content/drive/MyDrive/dataset_en.txt', 'w', encoding='utf-8')\n",
        "  for i in range(n):\n",
        "    f.write(data.iloc[i, 2] + '\\n')\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "x3dD4He3qSKi"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# 텍스트 토큰화 함수 정의\n",
        "def tokenize_text(text, sp_processor):\n",
        "    return sp_processor.encode_as_pieces(text)\n",
        "\n",
        "# 토큰 데이터 반환\n",
        "def make_token(file_path, save_path, token_model_prefix):\n",
        "\n",
        "  with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "  if not os.path.isfile(save_path):\n",
        "\n",
        "    with open(save_path, 'w', encoding='utf-8') as f:\n",
        "      for line in lines:\n",
        "        f.write(f'{line.strip()}\\n')\n",
        "\n",
        "    # SentencePiece 모델 학습\n",
        "  spm.SentencePieceTrainer.train(input=save_path, model_prefix=token_model_prefix, vocab_size=32000)\n",
        "\n",
        "  # SentencePiece 모델 로드\n",
        "  sp = spm.SentencePieceProcessor()\n",
        "  sp.load(token_model_prefix + '.model')\n",
        "\n",
        "  return [tokenize_text(line.strip(), sp) for line in lines]\n",
        "\n",
        "def make_index_list(word_list, vocab_file):\n",
        "  with open(vocab_file, 'r', encoding='utf-8') as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "  # 각 토큰과 해당 인덱스를 가져오기\n",
        "  token_to_index = {}\n",
        "  index_to_token = {}\n",
        "  for idx, line in enumerate(lines):\n",
        "      token = line.split(\"\\t\")[0]  # 토큰은 탭으로 구분된 첫 번째 요소입니다.\n",
        "      token_to_index[token] = idx\n",
        "      index_to_token[idx] = token\n",
        "\n",
        "  index_list = []\n",
        "  start, end = token_to_index['<s>'], token_to_index['</s>']\n",
        "  for word_list_item in word_list:\n",
        "    here = [token_to_index.get(token, token_to_index['<unk>']) for token in word_list_item]\n",
        "    here = [start] + here + [end]\n",
        "    index_list.append(here)\n",
        "  return token_to_index, index_to_token, index_list\n"
      ],
      "metadata": {
        "id": "WnJzRB3b5al0"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/dataset_ko.txt'\n",
        "save_path = '/content/drive/MyDrive/train_text_ko.txt'\n",
        "token_model_prefix = 'spm_ko_32000'\n",
        "\n",
        "token_ko = make_token(file_path, save_path, token_model_prefix)\n",
        "\n",
        "file_path = '/content/drive/MyDrive/dataset_en.txt'\n",
        "save_path = '/content/drive/MyDrive/train_text_en.txt'\n",
        "token_model_prefix = 'spm_en_32000'\n",
        "\n",
        "token_en = make_token(file_path, save_path, token_model_prefix)"
      ],
      "metadata": {
        "id": "yn3X7GStxCsI"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_to_index_ko, index_to_token_ko, index_list_ko = make_index_list(token_ko, 'spm_ko_32000.vocab')\n",
        "token_to_index_en, index_to_token_en, index_list_en = make_index_list(token_en, 'spm_en_32000.vocab')"
      ],
      "metadata": {
        "id": "gfBEjRxkKRWb"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index_list_ko[4])\n",
        "for i in index_list_ko[4]:\n",
        "  print(index_to_token_ko[i])\n",
        "\n",
        "print(index_list_en[4])\n",
        "for i in index_list_en[4]:\n",
        "  print(index_to_token_en[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grOXbH89NZpS",
        "outputId": "2f7643e4-b9ba-4acd-b1fa-b9b299f709f7"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 8, 21737, 12, 429, 12, 476, 915, 9, 171, 11565, 40, 21212, 4620, 1483, 2402, 1512, 3, 2]\n",
            "<s>\n",
            "▁\n",
            "6.5\n",
            ",\n",
            "▁7\n",
            ",\n",
            "▁8\n",
            "▁사이즈\n",
            "가\n",
            "▁몇\n",
            "▁개나\n",
            "▁더\n",
            "▁재입고\n",
            "▁될지\n",
            "▁제게\n",
            "▁알려주시면\n",
            "▁감사하겠습니다\n",
            ".\n",
            "</s>\n",
            "[1, 5, 69, 144, 1967, 6, 62, 143, 114, 3760, 28, 26, 7715, 13, 510, 14115, 7, 9575, 12, 810, 3, 2]\n",
            "<s>\n",
            "▁I\n",
            "▁would\n",
            "▁feel\n",
            "▁grateful\n",
            "▁to\n",
            "▁know\n",
            "▁how\n",
            "▁many\n",
            "▁stocks\n",
            "▁will\n",
            "▁be\n",
            "▁secured\n",
            "▁of\n",
            "▁size\n",
            "▁6.5\n",
            ",\n",
            "▁7,\n",
            "▁and\n",
            "▁8\n",
            ".\n",
            "</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# 토큰화된 데이터셋과 레이블 (예: ko_tokens, en_tokens)\n",
        "X = index_list_ko  # 입력 데이터\n",
        "y = index_list_en  # 레이블 데이터 (필요한 경우)\n",
        "\n",
        "# 데이터셋 정의\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, src, trg=None):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.trg is not None:\n",
        "            return self.src[idx], self.trg[idx]\n",
        "        else:\n",
        "            return self.src[idx]"
      ],
      "metadata": {
        "id": "ncJp83Jd-CKp"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_collate(batch):\n",
        "    # batch는 (data, label) 튜플의 리스트입니다.\n",
        "    # data와 label을 분리합니다.\n",
        "    src, trg = zip(*batch)\n",
        "    src = [torch.tensor(item) for item in src]\n",
        "    trg = [torch.tensor(item) for item in trg]\n",
        "    # src_trg = [torch.tensor(src), torch.tensor(trg)]\n",
        "    # data를 패딩하여 동일한 길이로 맞춥니다.\n",
        "    padding_value = 0\n",
        "    padded_src = pad_sequence(src, batch_first=True, padding_value=padding_value)\n",
        "    padded_trg = pad_sequence(trg, batch_first=True, padding_value=padding_value)\n",
        "\n",
        "    return padded_src, padded_trg"
      ],
      "metadata": {
        "id": "zn0aZCdGQWhF"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 190000\n",
        "valid_size = 5000\n",
        "\n",
        "# 학습, 검증, 테스트 데이터로 나누기\n",
        "train_data = CustomDataset(X[:train_size], y[:train_size])\n",
        "valid_data = CustomDataset(X[train_size:train_size+valid_size], y[train_size:train_size+valid_size])\n",
        "test_data =  CustomDataset(X[train_size+valid_size:], y[train_size+valid_size:])\n",
        "\n",
        "# DataLoader 정의\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=my_collate)\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=my_collate)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=my_collate)\n",
        "\n",
        "# 데이터 확인\n",
        "print(\"학습 배치 개수:\", len(train_loader))\n",
        "print(\"검증 배치 개수:\", len(valid_loader))\n",
        "print(\"테스트 배치 개수:\", len(test_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH8jZW3vBCKI",
        "outputId": "54477c61-9a35-459d-f028-a80dbc82b1e5"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 배치 개수: 1485\n",
            "검증 배치 개수: 40\n",
            "테스트 배치 개수: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 확인\n",
        "for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "    # 각 배치의 데이터 크기 확인\n",
        "    print(\"Batch\", batch_idx)\n",
        "    print(\"Data size:\", data.size())  # (배치 크기, 시퀀스 최대 길이)\n",
        "    print(\"Labels size:\", labels.size())  # (배치 크기,)\n",
        "    break\n",
        "\n",
        "for batch_idx, (data, labels) in enumerate(valid_loader):\n",
        "    # 각 배치의 데이터 크기 확인\n",
        "    print(\"Batch\", batch_idx)\n",
        "    print(\"Data size:\", data.size())  # (배치 크기, 시퀀스 최대 길이)\n",
        "    print(\"Labels size:\", labels.size())  # (배치 크기,)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TS8jE3oUDNvZ",
        "outputId": "d5ca9438-c79b-4c5b-eca4-1b610f99acaf"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0\n",
            "Data size: torch.Size([128, 29])\n",
            "Labels size: torch.Size([128, 35])\n",
            "Batch 0\n",
            "Data size: torch.Size([128, 31])\n",
            "Labels size: torch.Size([128, 39])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaled Dot-Product Attention 아키텍처"
      ],
      "metadata": {
        "id": "hTdlpTOn1sof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi-Head Attention 아키텍처\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* attention 입력 요소\n",
        "  *   query\n",
        "  * keys\n",
        "  *   values\n",
        "  *   \n",
        "*   hyperparameter\n",
        "  * hidden_dim\n",
        "\n"
      ],
      "metadata": {
        "id": "z1g09qbzOTme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "_84DMftn_1kc"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_heads, dropout_ratio=0.1, device=device):\n",
        "    super().__init__()\n",
        "\n",
        "    assert(hidden_dim % n_heads == 0)\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.n_heads = n_heads\n",
        "    self.n_heads_dim = hidden_dim // n_heads\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "    self.device = device\n",
        "\n",
        "    # linear_layer\n",
        "    self.fc_q = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
        "    self.fc_k = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
        "    self.fc_v = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
        "\n",
        "    self.fc = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio).to(device)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.n_heads_dim])).to(device)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "    batch_size = query.shape[0]\n",
        "\n",
        "    # Q : [batch_size, query_len, hidden_dim]\n",
        "    # K : [batch_size, key_len, hidden_dim]\n",
        "    # V : [batch_size, value_len, hidden_dim]\n",
        "    Q = self.fc_q(query).to(self.device)\n",
        "    K = self.fc_k(key).to(self.device)\n",
        "    V = self.fc_v(value).to(self.device)\n",
        "\n",
        "    # Q : [batch_size, n_heads, query_len, n_heads_dim]\n",
        "    # K : [batch_size, n_heads, key_len, n_heads_dim]\n",
        "    # V : [batch_size, n_heads, value_len, n_heads_dim]\n",
        "    Q = Q.view(batch_size, -1, self.n_heads, self.n_heads_dim).permute(0, 2, 1, 3).to(self.device)\n",
        "    K = K.view(batch_size, -1, self.n_heads, self.n_heads_dim).permute(0, 2, 1, 3).to(self.device)\n",
        "    V = V.view(batch_size, -1, self.n_heads, self.n_heads_dim).permute(0, 2, 1, 3).to(self.device)\n",
        "\n",
        "    # energy : [batch_size, n_heads, query_len, key_len]\n",
        "    energy = torch.matmul(Q, K.permute(0, 1, 3, 2)).to(self.device) / self.scale\n",
        "\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0, -1e10).to(self.device)\n",
        "\n",
        "    # attention : [batch_size, n_heads, query_len, key_len]\n",
        "    attention = torch.softmax(energy, dim=-1).to(self.device)\n",
        "\n",
        "    x = torch.matmul(self.dropout(attention), V).to(self.device)\n",
        "    # x = [batch_size, n_heads, query_len, n_heads_dim]\n",
        "\n",
        "    x = x.permute(0, 2, 1, 3).contiguous().to(self.device)\n",
        "    x = x.view(batch_size, -1, self.hidden_dim).to(self.device)\n",
        "    x = self.fc(x).to(self.device)\n",
        "    # x = [batch_size, query_len, hidden_dim]\n",
        "\n",
        "    return x ,attention"
      ],
      "metadata": {
        "id": "ut2NU_f1OXMb"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feedforward 아키텍처\n",
        "\n",
        "\n",
        "*   간단하게 feedforward\n",
        "*   입력 차원 = 출력차원\n",
        "\n"
      ],
      "metadata": {
        "id": "DPuT_agaGKdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, hidden_dim, fc_dim, dropout_ratio=0.1, device=device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.fc1 = nn.Linear(hidden_dim, fc_dim).to(device)\n",
        "    self.fc2 = nn.Linear(fc_dim, hidden_dim).to(device)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio).to(device)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(torch.relu(self.fc1(x)))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "T7GSYbY3GpPi"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EncoderLayer 아키텍처"
      ],
      "metadata": {
        "id": "JoI43WZDHxkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_heads, fc_dim, dropout_ratio=0.1, device=device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hidden_dim).to(device)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hidden_dim).to(device)\n",
        "    self.self_attn = MultiHeadAttention(hidden_dim, n_heads, dropout_ratio, device).to(device)\n",
        "    self.ff = FeedForward(hidden_dim, fc_dim, dropout_ratio, device).to(device)\n",
        "    self.dropout = nn.Dropout(dropout_ratio).to(device)\n",
        "\n",
        "  def forward(self, src, mask):\n",
        "    # src = [batch_size, src_len, hidden_dim]\n",
        "    # mask = [batch_size, src_len]\n",
        "\n",
        "    _out, _ = self.self_attn(src, src, src, mask=mask)\n",
        "    out = self.self_attn_layer_norm(src + self.dropout(_out))\n",
        "    _out = self.ff(out)\n",
        "    out = self.ff_layer_norm(out + self.dropout(_out))\n",
        "    #out = [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "gAao1tIIHzdK"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder 아키텍처"
      ],
      "metadata": {
        "id": "G6zEppwnLZTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, n_layers, n_heads, fc_dim, dropout_ratio=0.1, max_length=100, device=device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.enc_embedding = nn.Embedding(input_dim, hidden_dim).to(device)\n",
        "    self.pos_embedding = nn.Embedding(max_length, hidden_dim).to(device)\n",
        "\n",
        "    self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, fc_dim, dropout_ratio, device) for _ in range(n_layers)]).to(device)\n",
        "    self.dropout = nn.Dropout(dropout_ratio).to(device)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "  def forward(self, src, mask):\n",
        "    batch_size = src.shape[0]\n",
        "    src_len = src.shape[1]\n",
        "\n",
        "    pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "    pos = self.pos_embedding(pos).to(self.device)\n",
        "\n",
        "    src = self.dropout(self.enc_embedding(src) * self.scale + pos).to(self.device)\n",
        "    # src = [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      out = layer(src, mask)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "78gchAIqJrDO"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DecoderLayer 아키텍처"
      ],
      "metadata": {
        "id": "uOJLlh3yRsig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_heads, fc_dim, dropout_ratio=0.1, device=device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hidden_dim).to(device)\n",
        "    self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim).to(device)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hidden_dim).to(device)\n",
        "\n",
        "    self.self_attn = MultiHeadAttention(hidden_dim, n_heads, dropout_ratio, device)\n",
        "    self.enc_attn = MultiHeadAttention(hidden_dim, n_heads, dropout_ratio, device)\n",
        "    self.ff = FeedForward(hidden_dim, fc_dim, dropout_ratio)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio).to(device)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, trg, enc_out, src_mask, trg_mask):\n",
        "    # trg : [batch_size, trg_len, hidden_dim]\n",
        "    # enc_out : [batch_size, src_len, hidden_dim]\n",
        "    # src_mask : [batch_size, src_len]\n",
        "    # trg_mask : [batch_size, trg_len]\n",
        "\n",
        "    _trg, _ = self.self_attn(trg, trg, trg, mask=trg_mask)\n",
        "    trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    _trg, attention = self.enc_attn(trg, enc_out, enc_out, mask=src_mask)\n",
        "    trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    _trg = self.ff(trg)\n",
        "    trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    return trg, attention"
      ],
      "metadata": {
        "id": "-BTCfJxlRu-O"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, hidden_dim, n_layers, n_heads, fc_dim, dropout_ratio=0.1, max_length=100, device=device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.dec_embedding = nn.Embedding(output_dim, hidden_dim).to(device)\n",
        "    self.pos_embedding = nn.Embedding(max_length, hidden_dim).to(device)\n",
        "\n",
        "    self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, fc_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim).to(device)\n",
        "    self.dropout = nn.Dropout(dropout_ratio).to(device)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "  def forward(self, trg, enc_out, src_mask, trg_mask):\n",
        "    batch_size = trg.shape[0]\n",
        "    trg_len = trg.shape[1]\n",
        "\n",
        "    pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "    pos = self.pos_embedding(pos)\n",
        "\n",
        "    trg = self.dropout(self.dec_embedding(trg) * self.scale + pos)\n",
        "    # trg = [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      trg, attention = layer(trg, enc_out, src_mask, trg_mask)\n",
        "\n",
        "    out = self.fc(trg)\n",
        "    # trg = [batch_size, trg_len, output_dim]\n",
        "\n",
        "    return out, attention"
      ],
      "metadata": {
        "id": "_85oilHRY4qq"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer 아키텍처"
      ],
      "metadata": {
        "id": "Ujb5Ldf9aHYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device=device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2).to(self.device)\n",
        "    # src_mask : [batch_size, 1, 1, src_len]\n",
        "\n",
        "    return src_mask\n",
        "\n",
        "  def make_trg_mask(self, trg):\n",
        "    _, trg_len = trg.shape\n",
        "    trg_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "    # trg_mask : [batch_size, 1, trg_len, trg_len]\n",
        "    trg_mask_one = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2).to(self.device)\n",
        "    trg_mask = trg_mask & trg_mask_one\n",
        "    # trg_mask : [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "    return trg_mask\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "    # src_mask : [batch_size, 1, 1, src_len]\n",
        "    # trg_mask : [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "    enc_out = self.encoder(src, src_mask)\n",
        "    # enc_out : [batch_size, src_len, hidden_dim]\n",
        "\n",
        "    out, attention = self.decoder(trg, enc_out, src_mask, trg_mask)\n",
        "    # out : [batch_size, trg_len, output_dim]\n",
        "    # attention : [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "    return out, attention"
      ],
      "metadata": {
        "id": "DUerKZ_Sao8_"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#모델 training"
      ],
      "metadata": {
        "id": "YdDql76DZhDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(token_to_index_ko)\n",
        "OUTPUT_DIM = len(token_to_index_en)\n",
        "HIDDEN_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "MAX_LEGNTH=100 # 한 문장 단어 최대 개수"
      ],
      "metadata": {
        "id": "YPkJLR2RdR2r"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_PAD_IDX = 0\n",
        "TRG_PAD_IDX = 0\n",
        "\n",
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
        "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, MAX_LEGNTH, device)\n",
        "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, MAX_LEGNTH, device)\n",
        "\n",
        "# Transformer 객체 선언\n",
        "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "metadata": {
        "id": "PVti5cdKdUU_"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qronr3bDfV32",
        "outputId": "d51868fc-6716-4e63-8e88-9ab0be86eb4b"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 28,612,864 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wXm7-WxfVDW",
        "outputId": "ca28053f-bbfe-4b3e-df01-8378b6e6abda"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (enc_embedding): Embedding(32000, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attn): MultiHeadAttention(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attn): MultiHeadAttention(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attn): MultiHeadAttention(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (dec_embedding): Embedding(32000, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attn): MultiHeadAttention(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attn): MultiHeadAttention(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attn): MultiHeadAttention(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc): Linear(in_features=256, out_features=32000, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습(train) 함수\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train() # 학습 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # 전체 학습 데이터를 확인하며\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch[0].to(model.device)\n",
        "        trg = batch[1].to(model.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "        # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "        # output: [배치 크기, trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기, trg_len]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward() # 기울기(gradient) 계산\n",
        "\n",
        "        # 기울기(gradient) clipping 진행\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 전체 손실 값 계산\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "4SZLTk_jZkBr"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가(evaluate) 함수\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval() # 평가 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch[0].to(model.device)\n",
        "            trg = batch[1].to(model.device)\n",
        "\n",
        "            # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "            # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "            # output: [배치 크기, trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기, trg_len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # 전체 손실 값 계산\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "X1LZSdeRnDrz"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "# import torch._dynamo\n",
        "\n",
        "# Adam optimizer로 학습 최적화\n",
        "LEARNING_RATE = 0.0005\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "metadata": {
        "id": "JcVVk2yTlMjK"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#학습 시작"
      ],
      "metadata": {
        "id": "th5huSHktWDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "dxXJC4UUtSks"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "cnt = 0\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "    end_time = time.time() # 종료 시간 기록\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'transformer_korean_to_english.pt')\n",
        "        cnt=0\n",
        "    else:\n",
        "      cnt += 1\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')\n",
        "\n",
        "    if cnt == 2:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQZh6NrWtYs3",
        "outputId": "4e26acae-5960-4c7e-b64d-f5d12ade4caf"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 3m 36s\n",
            "\tTrain Loss: 4.081 | Train PPL: 59.181\n",
            "\tValidation Loss: 2.730 | Validation PPL: 15.339\n",
            "Epoch: 02 | Time: 3m 35s\n",
            "\tTrain Loss: 2.819 | Train PPL: 16.760\n",
            "\tValidation Loss: 2.290 | Validation PPL: 9.873\n",
            "Epoch: 03 | Time: 3m 34s\n",
            "\tTrain Loss: 2.316 | Train PPL: 10.133\n",
            "\tValidation Loss: 2.131 | Validation PPL: 8.422\n",
            "Epoch: 04 | Time: 3m 34s\n",
            "\tTrain Loss: 1.997 | Train PPL: 7.367\n",
            "\tValidation Loss: 2.066 | Validation PPL: 7.892\n",
            "Epoch: 05 | Time: 3m 34s\n",
            "\tTrain Loss: 1.767 | Train PPL: 5.850\n",
            "\tValidation Loss: 2.061 | Validation PPL: 7.855\n",
            "Epoch: 06 | Time: 3m 34s\n",
            "\tTrain Loss: 1.593 | Train PPL: 4.919\n",
            "\tValidation Loss: 2.082 | Validation PPL: 8.018\n",
            "Epoch: 07 | Time: 3m 36s\n",
            "\tTrain Loss: 1.459 | Train PPL: 4.302\n",
            "\tValidation Loss: 2.108 | Validation PPL: 8.232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#평가"
      ],
      "metadata": {
        "id": "RAIcu2pTTYd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# files.download('transformer_korean_to_english.pt')"
      ],
      "metadata": {
        "id": "43AeRNJQTWyB"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"우리 모두 오늘 하루 기분 좋게 생활해봐요.\"\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"spm_ko_32000.model\")\n",
        "\n",
        "tokens = sp.EncodeAsPieces(sentence)\n",
        "token_idx = [token_to_index_ko[token] for token in tokens]\n",
        "token_idx = [0] + token_idx + [0]\n",
        "print(token_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5NAHS7LwXHl",
        "outputId": "8f6c4420-cbf8-44f6-be1c-213525d82d11"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 23, 166, 52, 364, 1018, 2617, 647, 11893, 32, 3, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 번역(translation) 함수\n",
        "def translate_sentence(sentence, tokeninzer_model, model, device=device, max_len=50, logging=True):\n",
        "  model.eval() # 평가 모드\n",
        "\n",
        "  sp = spm.SentencePieceProcessor()\n",
        "  sp.Load(tokeninzer_model + \".model\")\n",
        "\n",
        "  # 문장을 subword 단위로 토크나이즈\n",
        "  tokens = sp.EncodeAsPieces(sentence)\n",
        "  src_indexes = [token_to_index_ko[token] for token in tokens]\n",
        "  src_indexes = [token_to_index_ko['<s>']] + src_indexes + [token_to_index_ko['</s>']]\n",
        "\n",
        "  print(src_indexes)\n",
        "  print(1)\n",
        "\n",
        "  src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "  # 소스 문장에 따른 마스크 생성\n",
        "  src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "  # 인코더(endocer)에 소스 문장을 넣어 출력 값 구하기\n",
        "  with torch.no_grad():\n",
        "      enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "  # 디코더의 첫번째 입력으로 사용할 <sos>\n",
        "  trg_indexes = [token_to_index_en['<s>']]\n",
        "\n",
        "  # 첫번째 디코더 입력으로\n",
        "\n",
        "  for i in range(max_len):\n",
        "      trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "      # 출력 문장에 따른 마스크 생성\n",
        "      trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          output, attention = model.decoder(trg_tensor, enc_src, src_mask, trg_mask)\n",
        "\n",
        "      # 출력 문장에서 가장 마지막 단어만 사용\n",
        "      pred_token = output.argmax(2)[:,-1].item()\n",
        "\n",
        "      if pred_token in [token_to_index_en['</s>']]:\n",
        "        break\n",
        "      trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
        "\n",
        "      # <eos>를 만나는 순간 끝\n",
        "\n",
        "\n",
        "  # 각 출력 단어 인덱스를 실제 단어로 변환\n",
        "  trg_tokens = [index_to_token_en[i] for i in trg_indexes]\n",
        "\n",
        "  # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
        "  return trg_tokens[1:], attention"
      ],
      "metadata": {
        "id": "mVK1yzKLTk2a"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install ipywidgets"
      ],
      "metadata": {
        "id": "MexZOgOWdWAY"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "text_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter a sentence',\n",
        "    description='Sentence:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "button = widgets.Button(\n",
        "    description='영어로 번역하기',\n",
        "    disabled=False,\n",
        "    button_style='',\n",
        "    tooltip='Click me',\n",
        "    icon='check'\n",
        ")\n",
        "\n",
        "def on_button_click(b):\n",
        "    sentence = text_input.value\n",
        "    translation, attn = translate_sentence(sentence, \"spm_ko_32000\", model, device)\n",
        "    clear_output(wait=True)\n",
        "    display(text_input, button)\n",
        "    print(\"모델 출력 결과:\", \" \".join(translation))\n",
        "\n",
        "button.on_click(on_button_click)\n",
        "display(text_input, button)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "ebdca4ad712643679bd8a9fb74675d84",
            "5135a6ab51a74fc591c99e8402256ca2",
            "46fca8cde343483cb89c26a3ba6cd3af",
            "dc0fe602d26c4ad19f81c6c4679fb65a",
            "71ee3478fd014677a43a8703c4ae6c79",
            "d9d23ac237ad4891ba311746d795b11c"
          ]
        },
        "id": "zADG0HRzdhUB",
        "outputId": "e8eb4a3b-a0b8-4103-ef49-0733a925c709"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='저는 지금 굉장히 배고프고 밥을 먹기를 원하는 상태입니다.', description='Sentence:', placeholder='Enter a sentence')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebdca4ad712643679bd8a9fb74675d84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='영어로 번역하기', icon='check', style=ButtonStyle(), tooltip='Click me')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc0fe602d26c4ad19f81c6c4679fb65a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 출력 결과: ▁I ' m ▁very ▁hungry ▁now , ▁and ▁I ' m ▁going ▁to ▁eat ▁something .\n"
          ]
        }
      ]
    }
  ]
}